# ------------------------------------------------------------
# AIM:
# Implement K-Nearest Neighbors algorithm on diabetes.csv dataset.
# Compute confusion matrix, accuracy, error rate, precision and recall.
# ------------------------------------------------------------

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# ------------------------------------------------------------
# Step 1: Load the dataset
# ------------------------------------------------------------
data = pd.read_csv("diabetes.csv")
print("First 5 rows of dataset:\n", data.head())

# ------------------------------------------------------------
# Step 2: Check for missing values
# ------------------------------------------------------------
print("\nMissing values in dataset:\n", data.isnull().sum())

# ------------------------------------------------------------
# Step 3: Handle invalid zero values
# Some columns have 0 which are unrealistic â€” replace with mean
# ------------------------------------------------------------
cols_to_replace = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for column in cols_to_replace:
    data[column].replace(0, np.nan, inplace=True)
    data[column].fillna(data[column].mean(), inplace=True)

# ------------------------------------------------------------
# Step 4: Split data into Features (X) and Target (Y)
# ------------------------------------------------------------
X = data.drop('Outcome', axis=1)
Y = data['Outcome']

# ------------------------------------------------------------
# Step 5: Train-Test Split
# ------------------------------------------------------------
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ------------------------------------------------------------
# Step 6: Feature Scaling (Important for distance-based algorithms like KNN)
# ------------------------------------------------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ------------------------------------------------------------
# Step 7: Initialize and Train KNN model
# ------------------------------------------------------------
k = 5  # You can tune this value
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, Y_train)

# ------------------------------------------------------------
# Step 8: Make Predictions
# ------------------------------------------------------------
Y_pred = knn.predict(X_test)

# ------------------------------------------------------------
# Step 9: Evaluate Model
# ------------------------------------------------------------
cm = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
error_rate = 1 - accuracy
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
f1 = f1_score(Y_test, Y_pred)

# ------------------------------------------------------------
# Step 10: Display Results
# ------------------------------------------------------------
print("\nConfusion Matrix:\n", cm)
print("\nAccuracy: {:.2f}%".format(accuracy * 100))
print("Error Rate: {:.2f}%".format(error_rate * 100))
print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1 Score: {:.2f}".format(f1))

# ------------------------------------------------------------
# Step 11: Visualize Confusion Matrix
# ------------------------------------------------------------
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title(f'Confusion Matrix (K={k})')
plt.xlabel('Predicted Label')
plt.ylabel('Actual Label')
plt.show()

# ------------------------------------------------------------
# Step 12: Tune 'K' Value and Observe Accuracy
# ------------------------------------------------------------
accuracy_scores = []

for k in range(1, 15):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, Y_train)
    Y_pred = knn.predict(X_test)
    acc = accuracy_score(Y_test, Y_pred)
    accuracy_scores.append(acc)

plt.figure(figsize=(8,5))
plt.plot(range(1,15), accuracy_scores, marker='o', linestyle='--')
plt.title('KNN Accuracy vs. K Value')
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

best_k = np.argmax(accuracy_scores) + 1
print(f"\nBest K value: {best_k} with Accuracy = {max(accuracy_scores)*100:.2f}%")
